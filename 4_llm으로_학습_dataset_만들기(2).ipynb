{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle1130/README.md/blob/main/4_llm%EC%9C%BC%EB%A1%9C_%ED%95%99%EC%8A%B5_dataset_%EB%A7%8C%EB%93%A4%EA%B8%B0(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 설치"
      ],
      "metadata": {
        "id": "a6Tx1JKc5gkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LLmxzvLY8eJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94779dfd-4f1c-4c66-c2b8-da095c88f08e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.16.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (3.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.1.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install bertopic\n",
        "!pip install -q transformers bertopic timesfm\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from google.colab import drive\n",
        "\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import pickle\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google drive mount"
      ],
      "metadata": {
        "id": "gr6DNyDZ5eLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/your_project_folder')\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hAswYjCtC027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b9ee83-af0d-4a63-b90d-9e9fa11ae328"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "json메시지 로딩"
      ],
      "metadata": {
        "id": "SkCJaWJM5dVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    with open('processed-batch-1.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # text가 공백인 메시지는 제외\n",
        "    return [msg for msg in data if msg['text'].strip()]\n",
        "\n",
        "messages = load_data()\n",
        "\n",
        "# 모든 메시지에 anchor_group을 미리 None으로 초기화\n",
        "# 이후 앵커 트래킹 로직에서 적절히 값이 들어가도록 함\n",
        "for msg in messages:\n",
        "    msg['anchor_group'] = None\n",
        "\n",
        "CHECKPOINT_FILE = \"cluster_progress.json\"  # 수정: 새 이름\n",
        "PROCESSED_FILE = 'processed_messages.json'"
      ],
      "metadata": {
        "id": "J8_TI8no-PIT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EEVE 모델 로드"
      ],
      "metadata": {
        "id": "X_4JizqO5cDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/eeve_model\"\n",
        "\n",
        "class ResourceManager:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    def __enter__(self):\n",
        "        self.model_gpu = self.model.to('cuda')\n",
        "        return self.model_gpu\n",
        "    def __exit__(self, *args):\n",
        "        self.model_gpu.to('cpu')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "class EEVEModel:\n",
        "    def __init__(self, model_path):\n",
        "        # 여기서 tokenizer, model 을 미리 로딩\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModel.from_pretrained(model_path).half().to('cpu')\n",
        "\n",
        "    def generate_embedding(self, text: str) -> np.ndarray:\n",
        "        # ResourceManager로 GPU 리소스를 할당받아서 임베딩 계산\n",
        "        with ResourceManager(self.model) as model_gpu:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\",\n",
        "                                    truncation=True,\n",
        "                                    max_length=512).to('cuda')\n",
        "            with torch.no_grad():\n",
        "                outputs = model_gpu(**inputs)\n",
        "            emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "        return emb"
      ],
      "metadata": {
        "id": "fMuYACHW-UEJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic 로드"
      ],
      "metadata": {
        "id": "1qAk4nnLJNhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 BERTopic 모델 불러오기 (Google Drive에 있는 trained_topic_model.pkl 사용)\n",
        "trained_topic_model = BERTopic.load(\"trained_topic_model.pkl\")\n",
        "\n",
        "# HybridSimilarity에서 사용할 BERTopic 모델 래퍼 클래스\n",
        "# transform([text], embeddings=[...])를 호출할 수 있도록 그대로 사용합니다.\n",
        "class LoadedBERTopic:\n",
        "    def transform(self, texts, embeddings=None):\n",
        "        # embeddings 인자를 전달하면 trained_topic_model.transform()에 함께 전달합니다.\n",
        "        # 모델의 transform()은 (topics, topic_dists)를 반환합니다.\n",
        "        return trained_topic_model.transform(texts, embeddings=embeddings)\n",
        "class LoadBERTopic:\n",
        "    def transform(self, texts):\n",
        "        \"\"\"\n",
        "        실제 BERTopic의 transform(texts) 결과로부터\n",
        "        topic id나 topic 분포를 얻어서 유사도 계산에 사용.\n",
        "        여기서는 단순히 0.7~0.8 사이 임의 값으로 가정\n",
        "        \"\"\"\n",
        "        # 예시로 topic distribution을 임의 반환\n",
        "        # 실제로는 bertopic_model.transform([...]) 형태로 topic 확률 벡터를 얻어야 함\n",
        "        return [None, [np.array([0.7, 0.3]) for _ in texts]]\n",
        "\n",
        "# BERTopic 내부에서 사용할 임베딩 모델도 동일하게 로딩해둬야 함 (예: sentence-transformers)\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "02JGJ4CnJTEM",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mention(호출) 검사 함수"
      ],
      "metadata": {
        "id": "qy9kKu9JJkDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mentions(text):\n",
        "    \"\"\"\n",
        "    '~님' 형태의 호출 패턴을 단순 정규식으로 추출\n",
        "    \"\"\"\n",
        "    found = re.findall(r\"([\\w-]+)님\", text)\n",
        "    return set(found)"
      ],
      "metadata": {
        "id": "vEAVjPlrJji1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Topic Similarity (EEVE+BERTopic)"
      ],
      "metadata": {
        "id": "QmUIbduZJpKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridSimilarity:\n",
        "    def __init__(self, eeve_model, bertopic_model, weight_eeve=0.5):\n",
        "        self.eeve_model = eeve_model\n",
        "        self.bertopic_model = bertopic_model  # LoadedBERTopic 인스턴스 전달\n",
        "        self.weight_eeve = weight_eeve\n",
        "\n",
        "    def cosine_sim(self, v1, v2):\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12)\n",
        "\n",
        "    def similarity(self, text, group_repr):\n",
        "        # EEVE 임베딩 유사도 계산\n",
        "        emb_text = self.eeve_model.generate_embedding(text)\n",
        "        sim_eeve = self.cosine_sim(emb_text, group_repr[\"embedding\"])\n",
        "\n",
        "        # BERTopic의 transform()으로 토픽 분포 계산\n",
        "        # embeddings 인자는 numpy array로 shape (1, vector_dim)이어야 함\n",
        "        emb_array = np.array([emb_text])\n",
        "        _, topic_dists = self.bertopic_model.transform([text], embeddings=emb_array)\n",
        "        sim_bertopic = self.cosine_sim(topic_dists[0], group_repr[\"topic_dist\"])\n",
        "\n",
        "        hybrid_sim = self.weight_eeve * sim_eeve + (1 - self.weight_eeve) * sim_bertopic\n",
        "        return hybrid_sim"
      ],
      "metadata": {
        "id": "Dm5wydrhJuCf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앵커 트래킹 클래스"
      ],
      "metadata": {
        "id": "WO7mx9d25aba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorTracker:\n",
        "    def __init__(self, eeve_path, threshold=0.5, weight_mention=0.8,\n",
        "                 merge_threshold=7, merge_similarity=0.8, time_threshold=86400):\n",
        "        self.eeve_model = EEVEModel(eeve_path)\n",
        "        self.bertopic_model = LoadedBERTopic()  # 실제 BERTopic 모델 불러오기\n",
        "        self.sim_model = HybridSimilarity(self.eeve_model, self.bertopic_model, weight_eeve=0.5)\n",
        "        self.groups = {}  # 그룹 정보 저장 (각 그룹은 {'embedding', 'topic_dist', 'tail_sender', 'tail_text', 'messages'})\n",
        "        self.current_id = 1\n",
        "        self.threshold = threshold  # (미사용; 아래 자체 기준으로 새 그룹/기존 그룹 선택)\n",
        "        self.weight_mention = weight_mention  # ← 호출 문구 점수를 조절하는 가중치 (예: 0.8)\n",
        "        self.time_threshold = time_threshold  # 예: 24시간(86400초)\n",
        "        self.merge_threshold = merge_threshold  # on-the-fly merge 수행 메시지 수 기준\n",
        "        self.merge_similarity = merge_similarity  # on-the-fly merge를 위한 주제 유사도 임계치\n",
        "\n",
        "    def context_mention_score(self, new_text, group_tail_sender):\n",
        "        # 앞/뒤 5개 단어 내에 group_tail_sender가 정확히 \"xxx님\" (호출)로 등장하면 4점,\n",
        "        # 단순히 텍스트에 포함되어 있으면 2점, 없으면 0점으로 평가합니다.\n",
        "        mention_score = 0\n",
        "        pattern_explicit = r\"\\b\" + re.escape(group_tail_sender) + r\"님\\b\"\n",
        "        if re.search(pattern_explicit, new_text):\n",
        "            mention_score = 4\n",
        "        elif group_tail_sender in new_text:\n",
        "            mention_score = 2\n",
        "        return mention_score\n",
        "\n",
        "    def assign_group(self, msg_text, msg_sender, anchor_possible=False):\n",
        "        current_time = datetime.now()\n",
        "        # EEVE 임베딩 및 BERTopic 토픽 분포 구하기\n",
        "        new_emb = self.eeve_model.generate_embedding(msg_text)\n",
        "        emb_array = np.array([new_emb])\n",
        "        _, new_topic_dists = self.bertopic_model.transform([msg_text], embeddings=emb_array)\n",
        "        new_topic = new_topic_dists[0]\n",
        "\n",
        "        # (1) 만약 메시지에 \"anchor_possible\": true이면 무조건 새 그룹 생성\n",
        "        if anchor_possible:\n",
        "            new_id = self.current_id\n",
        "            self.current_id += 1\n",
        "            self.groups[new_id] = {\n",
        "                \"embedding\": new_emb,\n",
        "                \"topic_dist\": new_topic,\n",
        "                \"tail_sender\": msg_sender,\n",
        "                \"tail_text\": msg_text,\n",
        "                \"messages\": [(msg_text, current_time, new_emb)]\n",
        "            }\n",
        "            return new_id\n",
        "\n",
        "        best_gid = None\n",
        "        best_total_score = -1\n",
        "\n",
        "        # (2) 기존 그룹들 순회: 단, 24시간 이내인 그룹만 고려\n",
        "        for gid, info in self.groups.items():\n",
        "            last_time = max(info[\"messages\"], key=lambda x: x[1])[1]\n",
        "            if (current_time - last_time).total_seconds() > self.time_threshold:\n",
        "                continue\n",
        "\n",
        "            # mention score: 현재 메시지에서 그룹의 tail_sender가 호출되었는지 확인\n",
        "            group_tail_sender = info[\"tail_sender\"]\n",
        "            mention_score = self.context_mention_score(msg_text, group_tail_sender)\n",
        "\n",
        "            # tail score: 기존 그룹의 마지막 메시지 임베딩과 new_emb의 cosine similarity → 최대 4점\n",
        "            tail_emb = info[\"messages\"][-1][2]\n",
        "            tail_sim = self.sim_model.cosine_sim(new_emb, tail_emb)\n",
        "            tail_score = 4 * tail_sim\n",
        "\n",
        "            # topic score: new message의 topic 분포와 그룹의 topic_dist 유사도 → 최대 2점\n",
        "            topic_sim = self.sim_model.cosine_sim(new_topic, info[\"topic_dist\"])\n",
        "            topic_score = 2 * topic_sim\n",
        "\n",
        "            total_score = mention_score + tail_score + topic_score\n",
        "            if total_score > best_total_score:\n",
        "                best_total_score = total_score\n",
        "                best_gid = gid\n",
        "\n",
        "        # 만약 최고 점수가 낮으면 새로운 그룹으로 처리 (예: 점수가 5점 미만이면)\n",
        "        if best_total_score < 5:\n",
        "            best_gid = None\n",
        "\n",
        "        if best_gid is not None:\n",
        "            # 기존 그룹 업데이트 (가중 평균으로 임베딩 및 topic 업데이트)\n",
        "            self.groups[best_gid][\"embedding\"] = 0.7 * self.groups[best_gid][\"embedding\"] + 0.3 * new_emb\n",
        "            self.groups[best_gid][\"topic_dist\"] = 0.7 * self.groups[best_gid][\"topic_dist\"] + 0.3 * new_topic\n",
        "            self.groups[best_gid][\"tail_sender\"] = msg_sender\n",
        "            self.groups[best_gid][\"tail_text\"] = msg_text\n",
        "            self.groups[best_gid][\"messages\"].append((msg_text, current_time, new_emb))\n",
        "            # on-the-fly merge 시도: 만약 해당 그룹 내 메시지 수가 merge_threshold 이상이면 병합 시도\n",
        "            if len(self.groups[best_gid][\"messages\"]) >= self.merge_threshold:\n",
        "                self.maybe_merge_group(best_gid)\n",
        "            return best_gid\n",
        "        else:\n",
        "            # 새 그룹 생성\n",
        "            new_id = self.current_id\n",
        "            self.current_id += 1\n",
        "            self.groups[new_id] = {\n",
        "                \"embedding\": new_emb,\n",
        "                \"topic_dist\": new_topic,\n",
        "                \"tail_sender\": msg_sender,\n",
        "                \"tail_text\": msg_text,\n",
        "                \"messages\": [(msg_text, current_time, new_emb)]\n",
        "            }\n",
        "            return new_id\n",
        "\n",
        "    def maybe_merge_group(self, current_gid):\n",
        "        \"\"\"\n",
        "        on-the-fly merge: 현재 그룹(current_gid)의 메시지 수가 merge_threshold 이상이면,\n",
        "        기존 그룹들 중 주제 유사도가 merge_similarity 이상인 그룹을 찾아 병합.\n",
        "        병합 시 임베딩과 topic_dist는 가중 평균으로 업데이트하고 tail 정보 재갱신.\n",
        "        \"\"\"\n",
        "        current_group = self.groups[current_gid]\n",
        "        for gid, other_group in list(self.groups.items()):\n",
        "            if gid == current_gid:\n",
        "                continue\n",
        "            topic_sim = np.dot(current_group[\"topic_dist\"], other_group[\"topic_dist\"]) / (\n",
        "                        np.linalg.norm(current_group[\"topic_dist\"]) * np.linalg.norm(other_group[\"topic_dist\"]) + 1e-12)\n",
        "            if topic_sim >= self.merge_similarity:\n",
        "                current_group[\"messages\"].extend(other_group[\"messages\"])\n",
        "                n1 = len(current_group[\"messages\"])\n",
        "                n2 = len(other_group[\"messages\"])\n",
        "                total = n1 + n2\n",
        "                current_group[\"embedding\"] = (n1 * current_group[\"embedding\"] + n2 * other_group[\"embedding\"]) / total\n",
        "                current_group[\"topic_dist\"] = (n1 * current_group[\"topic_dist\"] + n2 * other_group[\"topic_dist\"]) / total\n",
        "                latest_msg = max(current_group[\"messages\"], key=lambda x: x[1])\n",
        "                current_group[\"tail_text\"] = latest_msg[0]\n",
        "                del self.groups[gid]\n",
        "                self.maybe_merge_group(current_gid)\n",
        "\n",
        "# 후처리 merge 함수는 이전 코드와 같이 매우 엄격한 기준(topic_threshold=0.95)을 사용\n",
        "def merge_anchor_groups(anchor_groups, context_threshold=3600, topic_threshold=0.95):\n",
        "    \"\"\"\n",
        "    post-tracking merge: 24시간 내 tail message들 중, 엄격한 토픽 유사도(topic_threshold=0.95 이상)일 때만 그룹 병합.\n",
        "    \"\"\"\n",
        "    group_list = list(anchor_groups.items())\n",
        "    merged = {}\n",
        "    for gid, info in group_list:\n",
        "        if gid in merged:\n",
        "            continue\n",
        "        merged[gid] = info\n",
        "        tail_msg = sorted(info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "        for other_gid, other_info in group_list:\n",
        "            if other_gid == gid or (other_gid in merged and merged[other_gid] is None):\n",
        "                continue\n",
        "            other_tail = sorted(other_info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "            time_diff = abs(tail_msg[1] - other_tail[1]).total_seconds()\n",
        "            if time_diff > context_threshold:\n",
        "                continue\n",
        "            sim = np.dot(info[\"topic_dist\"], other_info[\"topic_dist\"]) / (\n",
        "                     np.linalg.norm(info[\"topic_dist\"]) * np.linalg.norm(other_info[\"topic_dist\"]) + 1e-12)\n",
        "            if sim >= topic_threshold:\n",
        "                merged[gid][\"messages\"].extend(other_info[\"messages\"])\n",
        "                updated_emb = 0.5 * (merged[gid][\"embedding\"] + other_info[\"embedding\"])\n",
        "                merged[gid][\"embedding\"] = updated_emb\n",
        "                merged[other_gid] = None\n",
        "        merged[gid][\"tail_text\"] = max(merged[gid][\"messages\"], key=lambda x: x[1])[0]\n",
        "    final = {k: v for k, v in merged.items() if v is not None}\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "PHjz7VuR-WB1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "체크포인트 메시지 로딩"
      ],
      "metadata": {
        "id": "jj5_VxBdNylr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"processed-batch-1.json\"):\n",
        "    with open(\"processed-batch-1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    messages = [msg for msg in data if msg[\"text\"].strip()]\n",
        "else:\n",
        "    messages = []\n",
        "\n",
        "for msg in messages:\n",
        "    if \"anchor_group\" not in msg:\n",
        "        msg[\"anchor_group\"] = None\n",
        "\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "else:\n",
        "    progress = {\"processed_idx\": 0, \"failed_ids\": []}\n",
        "\n",
        "# tracker 생성\n",
        "tracker = AnchorTracker(\n",
        "    eeve_path=\"/content/drive/MyDrive/eeve_model\",\n",
        "    threshold=0.5,\n",
        "    weight_mention=0.8,\n",
        "    time_threshold=86400,\n",
        "    merge_threshold=7,\n",
        "    merge_similarity=0.8  # on-the-fly merge 기준 (post-tracking merge는 별도 함수에서 topic_threshold=0.95 사용)\n",
        ")\n",
        "\n",
        "# 디버깅: 초기에 생성된 anchor 그룹 수 출력\n",
        "print(f\"초기 생성된 anchor 그룹 수: {len(tracker.groups)}\")\n"
      ],
      "metadata": {
        "id": "GyecwJhnNzxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7776d903-0bfc-4116-8365-5b85cf80214a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "초기 생성된 anchor 그룹 수: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 & 앵커 트래킹"
      ],
      "metadata": {
        "id": "70eg3bp35YkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메시지 로딩 및 체크포인트 처리 부분은 그대로 사용\n",
        "\n",
        "# tracker 생성\n",
        "tracker = AnchorTracker(\n",
        "    eeve_path=\"/content/drive/MyDrive/eeve_model\",\n",
        "    threshold=0.5,\n",
        "    time_threshold=86400,\n",
        "    merge_threshold=7,\n",
        "    merge_similarity=0.8  # on-the-fly merge 기준, 후처리 merge는 topic_threshold=0.95 사용\n",
        ")\n",
        "\n",
        "print(f\"초기 생성된 anchor 그룹 수: {len(tracker.groups)}\")\n",
        "\n",
        "for i in tqdm(range(progress[\"processed_idx\"], len(messages)), desc=\"임베딩 생성\"):\n",
        "    try:\n",
        "        if 'embedding' in messages[i] and messages[i].get('anchor_group') not in [None, 0]:\n",
        "            continue\n",
        "\n",
        "        emb = tracker.eeve_model.generate_embedding(messages[i]['text']).tolist()\n",
        "        messages[i]['embedding'] = emb\n",
        "\n",
        "        sender = messages[i]['sender']\n",
        "        text   = messages[i]['text']\n",
        "        anchor_possible = messages[i].get(\"anchor_possible\", False)\n",
        "\n",
        "        assigned_gid = tracker.assign_group(\n",
        "            msg_text=text,\n",
        "            msg_sender=sender,\n",
        "            anchor_possible=anchor_possible\n",
        "        )\n",
        "        messages[i]['anchor_group'] = assigned_gid\n",
        "\n",
        "        progress[\"processed_idx\"] = i + 1\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"메시지 {i+1} 처리 후, 현재 anchor 그룹 수: {len(tracker.groups)}\")\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(progress, f)\n",
        "            with open(PROCESSED_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(messages, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 @ {i}: {str(e)}\")\n",
        "        progress[\"failed_ids\"].append(i)\n",
        "        continue\n",
        "\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(progress, f)\n",
        "with open(PROCESSED_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(messages, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# 후처리 merge (엄격한 기준)\n",
        "merged_groups = merge_anchor_groups(tracker.groups, context_threshold=3600, topic_threshold=0.95)\n",
        "print(\"후처리 merge 후 최종 anchor 그룹 수:\", len(merged_groups))\n"
      ],
      "metadata": {
        "id": "-nYdH8lS-XNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "86dc99f7-2759-4b26-efe6-e71db5d769fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "초기 생성된 anchor 그룹 수: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:   4%|▍         | 10/263 [00:12<03:09,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 10 처리 후, 현재 anchor 그룹 수: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:   8%|▊         | 20/263 [00:17<01:51,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 20 처리 후, 현재 anchor 그룹 수: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  11%|█▏        | 30/263 [00:21<01:57,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 30 처리 후, 현재 anchor 그룹 수: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  15%|█▌        | 40/263 [00:28<02:42,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 40 처리 후, 현재 anchor 그룹 수: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  19%|█▉        | 50/263 [00:33<01:55,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 50 처리 후, 현재 anchor 그룹 수: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  23%|██▎       | 60/263 [00:38<01:58,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 60 처리 후, 현재 anchor 그룹 수: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  27%|██▋       | 70/263 [00:45<02:00,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 70 처리 후, 현재 anchor 그룹 수: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  30%|███       | 80/263 [00:50<01:41,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 80 처리 후, 현재 anchor 그룹 수: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  34%|███▍      | 90/263 [00:56<01:56,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 90 처리 후, 현재 anchor 그룹 수: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  38%|███▊      | 100/263 [01:02<01:33,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 100 처리 후, 현재 anchor 그룹 수: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  42%|████▏     | 110/263 [01:07<01:25,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 110 처리 후, 현재 anchor 그룹 수: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  46%|████▌     | 120/263 [01:14<01:41,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 120 처리 후, 현재 anchor 그룹 수: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  49%|████▉     | 130/263 [01:20<01:16,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 130 처리 후, 현재 anchor 그룹 수: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  53%|█████▎    | 139/263 [01:24<01:10,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 140 처리 후, 현재 anchor 그룹 수: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  57%|█████▋    | 149/263 [01:31<01:21,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 150 처리 후, 현재 anchor 그룹 수: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  60%|██████    | 159/263 [01:38<00:57,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 160 처리 후, 현재 anchor 그룹 수: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  65%|██████▍   | 170/263 [01:45<00:58,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 170 처리 후, 현재 anchor 그룹 수: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  68%|██████▊   | 180/263 [01:51<00:50,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 180 처리 후, 현재 anchor 그룹 수: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  72%|███████▏  | 189/263 [01:56<00:48,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 190 처리 후, 현재 anchor 그룹 수: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  75%|███████▌  | 198/263 [02:02<00:35,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에러 @ 197: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r임베딩 생성:  76%|███████▌  | 199/263 [02:02<00:34,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 200 처리 후, 현재 anchor 그룹 수: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  79%|███████▉  | 209/263 [02:08<00:28,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 210 처리 후, 현재 anchor 그룹 수: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  81%|████████▏ | 214/263 [02:12<00:33,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에러 @ 213: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  83%|████████▎ | 219/263 [02:15<00:25,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 220 처리 후, 현재 anchor 그룹 수: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  85%|████████▌ | 224/263 [02:17<00:21,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에러 @ 223: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  87%|████████▋ | 229/263 [02:20<00:18,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 230 처리 후, 현재 anchor 그룹 수: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  91%|█████████ | 239/263 [02:27<00:16,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 240 처리 후, 현재 anchor 그룹 수: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  95%|█████████▍| 249/263 [02:33<00:07,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 250 처리 후, 현재 anchor 그룹 수: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  98%|█████████▊| 259/263 [02:38<00:02,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메시지 260 처리 후, 현재 anchor 그룹 수: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성: 100%|██████████| 263/263 [02:42<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "후처리 merge 후 최종 anchor 그룹 수: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "anchor group merge"
      ],
      "metadata": {
        "id": "FWst5zAc5s1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_anchor_groups(anchor_groups, context_threshold=3600, topic_threshold=0.95):\n",
        "    \"\"\"\n",
        "    post-tracking merge: 매우 엄격한 토픽 유사도(topic_threshold=0.95 이상)일 때만 그룹 병합.\n",
        "    \"\"\"\n",
        "    group_list = list(anchor_groups.items())\n",
        "    merged = {}\n",
        "    for gid, info in group_list:\n",
        "        if gid in merged:\n",
        "            continue\n",
        "        merged[gid] = info\n",
        "        tail_msg = sorted(info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "        for other_gid, other_info in group_list:\n",
        "            if other_gid == gid or (other_gid in merged and merged[other_gid] is None):\n",
        "                continue\n",
        "            other_tail = sorted(other_info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "            time_diff = abs(tail_msg[1] - other_tail[1]).total_seconds()\n",
        "            if time_diff > context_threshold:\n",
        "                continue\n",
        "            sim = np.dot(info[\"topic_dist\"], other_info[\"topic_dist\"]) / (np.linalg.norm(info[\"topic_dist\"]) * np.linalg.norm(other_info[\"topic_dist\"]) + 1e-12)\n",
        "            if sim >= topic_threshold:\n",
        "                merged[gid][\"messages\"].extend(other_info[\"messages\"])\n",
        "                updated_emb = 0.5 * (merged[gid][\"embedding\"] + other_info[\"embedding\"])\n",
        "                merged[gid][\"embedding\"] = updated_emb\n",
        "                merged[other_gid] = None\n",
        "        # (추가: 엄격한 병합 후 최신 tail 정보 재갱신)\n",
        "        merged[gid][\"tail_text\"] = max(merged[gid][\"messages\"], key=lambda x: x[1])[0]\n",
        "    final = {k: v for k, v in merged.items() if v is not None}\n",
        "    return final\n",
        "\n",
        "# 후처리 merge를 엄격한 기준(topic_threshold=0.95)으로 실행\n",
        "merged_groups = merge_anchor_groups(tracker.groups, context_threshold=3600, topic_threshold=0.95)\n",
        "print(\"후처리 merge 후 최종 anchor 그룹 수:\", len(merged_groups))\n"
      ],
      "metadata": {
        "id": "3OJjHPeb5xtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a67d5a-379e-4294-c225-a885c42cad94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "후처리 merge 후 최종 anchor 그룹 수: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 결과 저장"
      ],
      "metadata": {
        "id": "8o44ZOwV5WP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_dict = defaultdict(list)\n",
        "for msg in messages:\n",
        "    # 여기서 KeyError가 발생하지 않도록 anchor_group이 None이 아닌지 확인\n",
        "    if msg['anchor_group'] is None:\n",
        "        msg['anchor_group'] = -1  # 혹은 기타 임시값\n",
        "    group_dict[msg['anchor_group']].append(msg['text'])\n",
        "\n",
        "with open('grouped_messages.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(group_dict, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "BjwqCguA-YBn"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}