{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle1130/README.md/blob/main/4_llm%EC%9C%BC%EB%A1%9C_%ED%95%99%EC%8A%B5_dataset_%EB%A7%8C%EB%93%A4%EA%B8%B0(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 설치"
      ],
      "metadata": {
        "id": "a6Tx1JKc5gkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LLmxzvLY8eJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91784ff3-9d1e-456b-cd4c-4708ce110ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.16.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (3.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.1.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install bertopic\n",
        "!pip install -q transformers bertopic timesfm\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from google.colab import drive\n",
        "\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import pickle\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google drive mount"
      ],
      "metadata": {
        "id": "gr6DNyDZ5eLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/your_project_folder')\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hAswYjCtC027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0438be0a-ff09-4efe-a5b5-fc3bfe7b93f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "json메시지 로딩"
      ],
      "metadata": {
        "id": "SkCJaWJM5dVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    with open('processed-batch-1.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # text가 공백인 메시지는 제외\n",
        "    return [msg for msg in data if msg['text'].strip()]\n",
        "\n",
        "messages = load_data()\n",
        "\n",
        "# 모든 메시지에 anchor_group을 미리 None으로 초기화\n",
        "# 이후 앵커 트래킹 로직에서 적절히 값이 들어가도록 함\n",
        "for msg in messages:\n",
        "    msg['anchor_group'] = None\n",
        "\n",
        "CHECKPOINT_FILE = \"cluster_progress.json\"  # 수정: 새 이름\n",
        "PROCESSED_FILE = 'processed_messages.json'"
      ],
      "metadata": {
        "id": "J8_TI8no-PIT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EEVE 모델 로드"
      ],
      "metadata": {
        "id": "X_4JizqO5cDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/eeve_model\"\n",
        "\n",
        "class ResourceManager:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    def __enter__(self):\n",
        "        self.model_gpu = self.model.to('cuda')\n",
        "        return self.model_gpu\n",
        "    def __exit__(self, *args):\n",
        "        self.model_gpu.to('cpu')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "class EEVEModel:\n",
        "    def __init__(self, model_path):\n",
        "        # 여기서 tokenizer, model 을 미리 로딩\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModel.from_pretrained(model_path).half().to('cpu')\n",
        "\n",
        "    def generate_embedding(self, text: str) -> np.ndarray:\n",
        "        # ResourceManager로 GPU 리소스를 할당받아서 임베딩 계산\n",
        "        with ResourceManager(self.model) as model_gpu:\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\",\n",
        "                                    truncation=True,\n",
        "                                    max_length=512).to('cuda')\n",
        "            with torch.no_grad():\n",
        "                outputs = model_gpu(**inputs)\n",
        "            emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "        return emb"
      ],
      "metadata": {
        "id": "fMuYACHW-UEJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic 로드"
      ],
      "metadata": {
        "id": "1qAk4nnLJNhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 BERTopic 모델 불러오기 (Google Drive에 있는 trained_topic_model.pkl 사용)\n",
        "trained_topic_model = BERTopic.load(\"trained_topic_model.pkl\")\n",
        "\n",
        "# HybridSimilarity에서 사용할 BERTopic 모델 래퍼 클래스\n",
        "# transform([text], embeddings=[...])를 호출할 수 있도록 그대로 사용합니다.\n",
        "class LoadedBERTopic:\n",
        "    def transform(self, texts, embeddings=None):\n",
        "        # embeddings 인자를 전달하면 trained_topic_model.transform()에 함께 전달합니다.\n",
        "        # 모델의 transform()은 (topics, topic_dists)를 반환합니다.\n",
        "        return trained_topic_model.transform(texts, embeddings=embeddings)\n",
        "class LoadBERTopic:\n",
        "    def transform(self, texts):\n",
        "        \"\"\"\n",
        "        실제 BERTopic의 transform(texts) 결과로부터\n",
        "        topic id나 topic 분포를 얻어서 유사도 계산에 사용.\n",
        "        여기서는 단순히 0.7~0.8 사이 임의 값으로 가정\n",
        "        \"\"\"\n",
        "        # 예시로 topic distribution을 임의 반환\n",
        "        # 실제로는 bertopic_model.transform([...]) 형태로 topic 확률 벡터를 얻어야 함\n",
        "        return [None, [np.array([0.7, 0.3]) for _ in texts]]\n",
        "\n",
        "# BERTopic 내부에서 사용할 임베딩 모델도 동일하게 로딩해둬야 함 (예: sentence-transformers)\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "02JGJ4CnJTEM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mention(호출) 검사 함수"
      ],
      "metadata": {
        "id": "qy9kKu9JJkDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mentions(text):\n",
        "    \"\"\"\n",
        "    '~님' 형태의 호출 패턴을 단순 정규식으로 추출\n",
        "    \"\"\"\n",
        "    found = re.findall(r\"([\\w-]+)님\", text)\n",
        "    return set(found)"
      ],
      "metadata": {
        "id": "vEAVjPlrJji1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Topic Similarity (EEVE+BERTopic)"
      ],
      "metadata": {
        "id": "QmUIbduZJpKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridSimilarity:\n",
        "    def __init__(self, eeve_model, bertopic_model, weight_eeve=0.5):\n",
        "        self.eeve_model = eeve_model\n",
        "        self.bertopic_model = bertopic_model  # LoadedBERTopic 인스턴스 전달\n",
        "        self.weight_eeve = weight_eeve\n",
        "\n",
        "    def cosine_sim(self, v1, v2):\n",
        "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12)\n",
        "\n",
        "    def similarity(self, text, group_repr):\n",
        "        # EEVE 임베딩 유사도 계산\n",
        "        emb_text = self.eeve_model.generate_embedding(text)\n",
        "        sim_eeve = self.cosine_sim(emb_text, group_repr[\"embedding\"])\n",
        "\n",
        "        # BERTopic의 transform()으로 토픽 분포 계산\n",
        "        # embeddings 인자는 numpy array로 shape (1, vector_dim)이어야 함\n",
        "        emb_array = np.array([emb_text])\n",
        "        _, topic_dists = self.bertopic_model.transform([text], embeddings=emb_array)\n",
        "        sim_bertopic = self.cosine_sim(topic_dists[0], group_repr[\"topic_dist\"])\n",
        "\n",
        "        hybrid_sim = self.weight_eeve * sim_eeve + (1 - self.weight_eeve) * sim_bertopic\n",
        "        return hybrid_sim"
      ],
      "metadata": {
        "id": "Dm5wydrhJuCf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앵커 트래킹 클래스"
      ],
      "metadata": {
        "id": "WO7mx9d25aba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorTracker:\n",
        "    def __init__(self, eeve_path, threshold=0.5, weight_mention=0.8, time_threshold=86400):\n",
        "        self.eeve_model = EEVEModel(eeve_path)\n",
        "        self.bertopic_model = LoadedBERTopic()  # 실제 BERTopic 모델 인스턴스\n",
        "        self.sim_model = HybridSimilarity(self.eeve_model, self.bertopic_model, weight_eeve=0.5)\n",
        "        self.groups = {}  # 그룹별 정보 저장\n",
        "        self.current_id = 1\n",
        "        self.threshold = threshold\n",
        "        self.weight_mention = weight_mention\n",
        "        self.time_threshold = time_threshold  # 추가\n",
        "\n",
        "    def context_mention_score(self, new_text, new_sender, tail_sender):\n",
        "        score = 0.0\n",
        "        mentions = extract_mentions(new_text)\n",
        "        if tail_sender in mentions:\n",
        "            score = 1.0\n",
        "        return score\n",
        "\n",
        "    def assign_group(self, msg_text, msg_sender):\n",
        "        best_gid = None\n",
        "        best_score = -1.0\n",
        "\n",
        "        for gid, info in self.groups.items():\n",
        "            base_sim = self.sim_model.similarity(msg_text, info)\n",
        "            mention_sc = self.context_mention_score(msg_text, msg_sender, info[\"tail_sender\"])\n",
        "            # 최종 점수: mention 점수에 weight_mention 비중 부여\n",
        "            total_score = mention_sc * self.weight_mention + base_sim * (1 - self.weight_mention)\n",
        "            if total_score > best_score:\n",
        "                best_score = total_score\n",
        "                best_gid = gid\n",
        "\n",
        "        if best_gid is not None and best_score >= self.threshold:\n",
        "            # 기존 그룹 업데이트: 새로운 임베딩과 토픽 분포 업데이트\n",
        "            emb_new = self.eeve_model.generate_embedding(msg_text)\n",
        "            self.groups[best_gid][\"embedding\"] = 0.7 * self.groups[best_gid][\"embedding\"] + 0.3 * emb_new\n",
        "            self.groups[best_gid][\"tail_sender\"] = msg_sender\n",
        "            self.groups[best_gid][\"tail_text\"] = msg_text\n",
        "\n",
        "            emb_array = np.array([emb_new])\n",
        "            _, topic_dists = self.bertopic_model.transform([msg_text], embeddings=emb_array)\n",
        "            self.groups[best_gid][\"topic_dist\"] = 0.7 * self.groups[best_gid][\"topic_dist\"] + 0.3 * topic_dists[0]\n",
        "            return best_gid\n",
        "        else:\n",
        "            # 새 그룹 생성\n",
        "            new_id = self.current_id\n",
        "            self.current_id += 1\n",
        "            emb_new = self.eeve_model.generate_embedding(msg_text)\n",
        "            emb_array = np.array([emb_new])\n",
        "            _, topic_dists = self.bertopic_model.transform([msg_text], embeddings=emb_array)\n",
        "            self.groups[new_id] = {\n",
        "                \"embedding\": emb_new,\n",
        "                \"topic_dist\": topic_dists[0],\n",
        "                \"tail_sender\": msg_sender,\n",
        "                \"tail_text\": msg_text\n",
        "            }\n",
        "            return new_id"
      ],
      "metadata": {
        "id": "PHjz7VuR-WB1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "체크포인트 메시지 로딩"
      ],
      "metadata": {
        "id": "jj5_VxBdNylr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 메시지 로딩\n",
        "if os.path.exists(\"processed-batch-1.json\"):\n",
        "    with open(\"processed-batch-1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    # text가 공백인 메시지는 제외(예시)\n",
        "    messages = [msg for msg in data if msg[\"text\"].strip()]\n",
        "else:\n",
        "    messages = []\n",
        "\n",
        "# 모든 메시지 anchor_group 초기화\n",
        "for msg in messages:\n",
        "    if \"anchor_group\" not in msg:\n",
        "        msg[\"anchor_group\"] = None\n",
        "\n",
        "# 기존 진행상황 로드\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "else:\n",
        "    progress = {\"processed_idx\": 0, \"failed_ids\": []}\n",
        "\n",
        "tracker = AnchorTracker(\n",
        "    eeve_path=\"/content/drive/MyDrive/eeve_model\",\n",
        "    threshold=0.5,\n",
        "    weight_mention=0.8,\n",
        "    time_threshold=86400\n",
        ")\n"
      ],
      "metadata": {
        "id": "GyecwJhnNzxz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 & 앵커 트래킹"
      ],
      "metadata": {
        "id": "70eg3bp35YkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(progress[\"processed_idx\"], len(messages)), desc=\"임베딩 생성\"):\n",
        "    try:\n",
        "        # 이미 임베딩과 anchor_group 있으면 넘어감\n",
        "        if 'embedding' in messages[i] and messages[i].get('anchor_group') not in [None, 0]:\n",
        "            continue\n",
        "\n",
        "        # 임베딩 생성\n",
        "        emb = tracker.eeve_model.generate_embedding(messages[i]['text']).tolist()\n",
        "        messages[i]['embedding'] = emb\n",
        "\n",
        "        # 앵커 그룹 할당\n",
        "        # embedding_np와 msg_time 인자는 더 이상 전달하지 않습니다.\n",
        "        sender = messages[i]['sender']\n",
        "        text   = messages[i]['text']\n",
        "        assigned_gid = tracker.assign_group(\n",
        "            msg_text=text,\n",
        "            msg_sender=sender\n",
        "        )\n",
        "        messages[i]['anchor_group'] = assigned_gid\n",
        "\n",
        "        progress[\"processed_idx\"] = i + 1\n",
        "\n",
        "        # 10개 단위로 임시 저장\n",
        "        if (i + 1) % 10 == 0:\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(progress, f)\n",
        "            with open(PROCESSED_FILE, 'w', encoding='utf-8') as f:\n",
        "                json.dump(messages, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 @ {i}: {str(e)}\")\n",
        "        progress[\"failed_ids\"].append(i)\n",
        "        continue\n",
        "\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(progress, f)\n",
        "with open(PROCESSED_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(messages, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "-nYdH8lS-XNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4df0b576-8b5f-4476-ecb1-d4ed044534f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성:  54%|█████▍    | 142/263 [51:16<1:28:36, 43.94s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "anchor group merge"
      ],
      "metadata": {
        "id": "FWst5zAc5s1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
        "\n",
        "def merge_anchor_groups(anchor_groups, context_threshold=3600, topic_threshold=0.8):\n",
        "    \"\"\"\n",
        "    anchor_groups 형식 (예시):\n",
        "    {\n",
        "      group_id: {\n",
        "        \"messages\": [ (text, timestamp, embedding), ... ],\n",
        "        \"group_embedding\": np.array([...])     # 그룹 전체를 대표하는 임베딩\n",
        "      },\n",
        "      ...\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    group_list = list(anchor_groups.items())  # (group_id, group_info) 튜플 목록\n",
        "    merged = {}\n",
        "\n",
        "    for gid, info in group_list:\n",
        "        # 이미 merge로 흡수된 그룹 제외\n",
        "        if gid in merged:\n",
        "            continue\n",
        "        if gid not in merged:\n",
        "            merged[gid] = info\n",
        "\n",
        "        # tail 메시지(가장 마지막 메시지) 조회\n",
        "        tail_msg = sorted(info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "        tail_time = tail_msg[1]\n",
        "        tail_emb = tail_msg[2]\n",
        "\n",
        "        # 나머지 그룹들과 비교\n",
        "        for other_gid, other_info in group_list:\n",
        "            if other_gid == gid or other_gid in merged and merged[other_gid] is None:\n",
        "                continue\n",
        "\n",
        "            # 다른 그룹 tail\n",
        "            other_tail = sorted(other_info[\"messages\"], key=lambda x: x[1])[-1]\n",
        "            other_tail_time = other_tail[1]\n",
        "            other_tail_emb = other_tail[2]\n",
        "\n",
        "            # 맥락(시간) 연속성 평가: 두 tail 메시지 시간 차 체크\n",
        "            time_diff = abs(tail_time - other_tail_time).total_seconds()\n",
        "            if time_diff > context_threshold:\n",
        "                continue\n",
        "\n",
        "            # 토픽(주제) 유사도 평가\n",
        "            sim = compute_similarity(info[\"group_embedding\"], other_info[\"group_embedding\"])\n",
        "            if sim >= topic_threshold:\n",
        "                # 병합 처리\n",
        "                merged[gid][\"messages\"].extend(other_info[\"messages\"])\n",
        "                # 그룹 전체 임베딩 업데이트(간단히 평균)\n",
        "                updated_emb = 0.5 * (merged[gid][\"group_embedding\"] + other_info[\"group_embedding\"])\n",
        "                merged[gid][\"group_embedding\"] = updated_emb\n",
        "                # other_gid를 소멸 처리\n",
        "                merged[other_gid] = None\n",
        "\n",
        "    # 최종적으로 None 아닌 그룹만 추려서 반환\n",
        "    final = {}\n",
        "    for k, v in merged.items():\n",
        "        if v is not None:\n",
        "            final[k] = v\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "3OJjHPeb5xtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 결과 저장"
      ],
      "metadata": {
        "id": "8o44ZOwV5WP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_dict = defaultdict(list)\n",
        "for msg in messages:\n",
        "    # 여기서 KeyError가 발생하지 않도록 anchor_group이 None이 아닌지 확인\n",
        "    if msg['anchor_group'] is None:\n",
        "        msg['anchor_group'] = -1  # 혹은 기타 임시값\n",
        "    group_dict[msg['anchor_group']].append(msg['text'])\n",
        "\n",
        "with open('grouped_messages.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(group_dict, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "BjwqCguA-YBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}